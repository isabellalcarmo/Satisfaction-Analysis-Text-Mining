{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from torch import nn, optim\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification, XLMRobertaForTokenClassification \n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from gensim.utils import simple_preprocess\n",
    "from unidecode import unidecode\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments = open('commentsBrazilNormalized.txt', 'r', encoding=\"utf8\")\n",
    "# dates = open('datesBrazil.txt', 'r', encoding=\"utf8\")\n",
    "# grades = open('gradesBrazil.txt', 'r', encoding=\"utf8\")\n",
    "# titles = open('titlesBrazil.txt', 'r', encoding=\"utf8\")\n",
    "comments = open('comments/comments_grand_hyatt_sao_paulo.txt', 'r', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_data = comments.readlines()\n",
    "# dates_data = dates.readlines()\n",
    "# grades_data = grades.readlines()\n",
    "# titles_data = titles.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.close()\n",
    "# dates.close()\n",
    "# grades.close()\n",
    "# titles.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_list = [comment.strip() for comment in comments_data]\n",
    "# dates_list = [date.strip() for date in dates_data]\n",
    "# grades_list = [grade.strip() for grade in grades_data]\n",
    "# titles_list = [title.strip() for title in titles_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_comments = []\n",
    "# hotel_indexes = []\n",
    "\n",
    "# for i, comment in enumerate(comments_list):\n",
    "#     comment_without_accents = unidecode(comment)\n",
    "#     if re.search(r'\\bcullinam\\b', comment_without_accents, re.IGNORECASE):\n",
    "#         filtered_comments.append(comment)\n",
    "#         hotel_indexes.append(i)\n",
    "\n",
    "# print(len(filtered_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hotel_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments_list[569105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_comments = []\n",
    "\n",
    "# saint moritz: range(299, 1378) BRASÍLIA\n",
    "# grand hyatt: range(462126, 462913) RIO, range(556258, 558881) SÃO PAULO\n",
    "# royal tulip: range(11582, 13657) BRASÍLIA, range(387099, 387633) RIO\n",
    "# copacabana palace: range(419735, 421697) RIO\n",
    "# tivoli: range(566513, 567748) SÃO PAULO\n",
    "\n",
    "# for index in range(566513, 567748):\n",
    "#     selected_comments.append(comments_list[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(selected_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('comments_tivoli_sao_paulo.txt', 'w') as file:\n",
    "#     for comment in selected_comments:\n",
    "#         file.write(comment + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(list(zip(selected_comments, dates_list, grades_list, titles_list)), columns=['comments', 'dates', 'grades', 'titles'])\n",
    "df = pd.DataFrame(list(zip(comments_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 175\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download([\"stopwords\", \"rslp\"])\n",
    "\n",
    "PT_STOPWORDS = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "REGEX = [\n",
    "    {'input': r'/\\d+/g', 'output': r' '},               # removing digits\n",
    "    {'input': r'[^\\w\\s]', 'output': r' '},              # removing punctuationon\n",
    "    {'input': r'\\n', 'output': r' '},                   # removing line breaks\n",
    "    {'input': r'\\b\\w{1,2}\\b', 'output': r''},           # removing short words (1 or 2 characters)\n",
    "]\n",
    "\n",
    "def regex_treatment(text):\n",
    "    for pattern in REGEX:\n",
    "        text = re.sub(pattern['input'], pattern['output'], text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = simple_preprocess(text)\n",
    "    phrase_adjusted = \" \".join([word for word in words if word not in PT_STOPWORDS])\n",
    "    return phrase_adjusted.lower()\n",
    "\n",
    "spacy_lemma = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "# Ignoring VERBs, ADVs, ADJs, PROPNs (...)\n",
    "def lemmatizer(text, postags_permit=['NOUN']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    doc = spacy_lemma(text.lower())\n",
    "    doc_lemma = \" \".join([token.lemma_ for token in doc if token.pos_ in postags_permit])\n",
    "    return doc_lemma\n",
    "\n",
    "def remove_accentuation(text):\n",
    "    return unidecode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"comments_topic\"] = df[\"comments\"].apply(regex_treatment)\n",
    "df[\"comments_topic\"] = df[\"comments_topic\"].apply(remove_stopwords)\n",
    "df[\"comments_topic\"] = df[\"comments_topic\"].apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"comments_topic\"] = df[\"comments_topic\"].apply(remove_accentuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accentuation_lower(text):\n",
    "    return unidecode(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"comments_entity\"] = df[\"comments\"].apply(remove_accentuation_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_text = ' '.join(df[\"comments_topic\"].astype(str))\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, random_state=15, max_font_size=110, max_words=130, collocations=False).generate(wordcloud_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = df[\"comments_topic\"]\n",
    "doc_comments = df_comments.to_list()\n",
    "doc_comments[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df_hotel.csv\")\n",
    "df_comments.to_csv(\"df_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(df):\n",
    "    all_entities = []\n",
    "\n",
    "    for comment in df[\"comments_entity\"]:\n",
    "        predictions = ner_task(comment)\n",
    "        comment_entities = []\n",
    "\n",
    "        if predictions != []:\n",
    "            combined_word = \"\"\n",
    "            previous_entity = None\n",
    "\n",
    "            for prediction in predictions:\n",
    "                entity = prediction['entity']\n",
    "                word = prediction['word']\n",
    "\n",
    "                if not word.startswith('▁'):\n",
    "                    combined_word += word\n",
    "                else:\n",
    "                    if combined_word:\n",
    "                        comment_entities.append((combined_word, previous_entity))\n",
    "                    combined_word = word\n",
    "\n",
    "                previous_entity = entity\n",
    "\n",
    "            if combined_word:\n",
    "                comment_entities.append((combined_word, previous_entity))\n",
    "\n",
    "        all_entities.append(comment_entities)\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"token_predictions\"] = all_entities\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner = df.copy()\n",
    "df_ner[\"token_predictions\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, XLMRobertaForTokenClassification, pipeline, DistilBertTokenizerFast, BertForTokenClassification\n",
    "\n",
    "MAX_LEN = 175\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "tokenizer_ner = AutoTokenizer.from_pretrained(\"51la5/roberta-large-NER\", model_max_length=512)\n",
    "model_ner = XLMRobertaForTokenClassification.from_pretrained(\"51la5/roberta-large-NER\")\n",
    "ner_task = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner = extract_entities(df_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_unique = df_ner['token_predictions'].explode().value_counts()\n",
    "count_unique"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ninha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
