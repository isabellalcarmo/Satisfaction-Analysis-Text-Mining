{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from bertopic import BERTopic\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "nltk.download([\"stopwords\", \"rslp\"])\n",
    "stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "\n",
    "from transformers import AutoTokenizer  # Or BertTokenizer\n",
    "from transformers import AutoModelForSequenceClassification  # Or BertForPreTraining for loading pretraining heads\n",
    "from transformers import AutoModel  # or BertModel, for BERT without pretraining heads\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leitura do arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv(\"guacamole_reviews.csv\")\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = df_reviews[\"Review\"]\n",
    "df_dates = df_reviews[\"Date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpeza dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    words = simple_preprocess(text)\n",
    "    phrase_adjusted = \" \".join([word for word in words if word not in stopwords])\n",
    "    return phrase_adjusted.lower()\n",
    "\n",
    "spacy_lemma = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def lemmatizer(text, postags_permit=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    doc = spacy_lemma(text.lower())\n",
    "    doc_lemma = \" \".join([token.lemma_ for token in doc if token.pos_ in postags_permit])\n",
    "    return doc_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date(date_str):\n",
    "    months = {\n",
    "        'janeiro': 'January', 'fevereiro': 'February', 'março': 'March', 'abril': 'April',\n",
    "        'maio': 'May', 'junho': 'June', 'julho': 'July', 'agosto': 'August',\n",
    "        'setembro': 'September', 'outubro': 'October', 'novembro': 'November', 'dezembro': 'December'\n",
    "    }\n",
    "    match = re.match(r'(\\d+) de (\\w+) de (\\d+)', date_str)\n",
    "    if match:\n",
    "        day, month, year = match.groups()\n",
    "        month = months[month.lower()]\n",
    "        return datetime(int(year), list(months.values()).index(month) + 1, int(day)).strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"Review Lemma\"] = df_reviews[\"Review\"].map(remove_stopwords)\n",
    "df_reviews[\"Review Lemma\"] = df_reviews[\"Review\"].map(lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"Date Formatted\"] = df_reviews[\"Date\"].apply(parse_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = df_reviews[\"Review Lemma\"]\n",
    "doc_comments = df_comments.to_list()\n",
    "doc_comments[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(language=\"portuguese\", calculate_probabilities=True, verbose=True)\n",
    "topics, probs = topic_model.fit_transform(doc_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info(); freq.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualização dos tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_reviews = df_reviews[\"Date Formatted\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(doc_comments, dates_reviews, datetime_format=\"%Y-%m-%d\", nr_bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame com informações completas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = df_reviews.copy()\n",
    "\n",
    "df_topics[\"Topics\"] = topics\n",
    "\n",
    "topic_name = freq.drop(columns=[\"Count\"]).rename(columns={\"Topic\": \"Topics\", \"Name\": \"Names\"})\n",
    "df_topics = df_topics.merge(topic_name, how=\"left\")\n",
    "\n",
    "df_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de Sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('lxyuan/distilbert-base-multilingual-cased-sentiments-student')\n",
    "tokenizer = AutoTokenizer.from_pretrained('lxyuan/distilbert-base-multilingual-cased-sentiments-student', do_lower_case=False)\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_task(\"Eu sou feliz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificação das avaliações e salvamento dos resultados no DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_topics[\"Sentiment\"] = df_topics[\"Review\"].apply(lambda x: sentiment_task(x)[0][\"label\"])\n",
    "df_topics[\"Sentiment Score\"] = df_topics[\"Review\"].apply(lambda x: sentiment_task(x)[0][\"score\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics.to_csv('guacamole_topics_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_sentiment = df_topics[\"Sentiment\"].value_counts()\n",
    "count_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração de Entidade Nomeada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ner = AutoModelForTokenClassification.from_pretrained('51la5/roberta-large-NER')\n",
    "tokenizer_ner = AutoTokenizer.from_pretrained('51la5/roberta-large-NER', do_lower_case=False)\n",
    "ner_task = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_task(\"Julia não gosta de Londres nem Berlim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_ner(text):\n",
    "    \"\"\"\n",
    "    Token classification function using a pretrained model.\n",
    "\n",
    "    Parameters:\n",
    "    - text: Input text to be tokenized and classified.\n",
    "\n",
    "    Returns:\n",
    "    List of tuples containing predicted pairs (token, label) for the input text.\n",
    "\n",
    "    Example:\n",
    "    Input:  \"Julia is tired of living in London.\"\n",
    "    Output: [('Julia', 'B-PESSOA'), ('is', 'O'), ('tired', 'O'), ('of', 'O'),\n",
    "            ('living', 'O'), ('in', 'O'), ('London', 'B-LOCAL'), ('.', 'O')]\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer_ner(text, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    tokens = inputs.tokens()\n",
    "\n",
    "    outputs = model_ner(**inputs).logits\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "        label = model_ner.config.id2label.get(prediction, 'O')\n",
    "        if label != 'O':\n",
    "            results.append((token, label))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics[\"Token Predictions\"] = df_topics[\"Review\"].apply(classify_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics[\"Token Predictions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens(token_predictions):\n",
    "    \"\"\"\n",
    "    Function to merge consecutive tokens that start with \"_\" and have the same label.\n",
    "\n",
    "    Parameters:\n",
    "    - token_predictions: List of tuples containing predicted pairs (token, label).\n",
    "\n",
    "    Returns:\n",
    "    List of merged tuples where consecutive tokens starting with \"_\"\n",
    "    and having the same label are combined.\n",
    "\n",
    "    Example:\n",
    "    Input:  [('▁Mathe', 'I-PER'), ('us', 'I-PER')]\n",
    "    Output: [('▁Matheus', 'I-PER')]\n",
    "    \"\"\"\n",
    "\n",
    "    merged_results = []\n",
    "    current_token = \"\"\n",
    "    current_label = \"\"\n",
    "\n",
    "    for token, label in token_predictions:\n",
    "        if token.startswith(\"▁\"):\n",
    "            if current_token:\n",
    "                merged_results.append((current_token, current_label))\n",
    "            current_token = token[1:]\n",
    "            current_label = label\n",
    "        else:\n",
    "            current_token += token\n",
    "            current_label = label\n",
    "\n",
    "    if current_token:\n",
    "        merged_results.append((current_token, current_label))\n",
    "\n",
    "    return merged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics[\"Token Predictions Corrected\"] = df_topics[\"Token Predictions\"].apply(merge_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics[\"Token Predictions Corrected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_predictions_corrected = df_topics[\"Token Predictions Corrected\"]\n",
    "\n",
    "flat_list = [item for sublist in token_predictions_corrected for item in sublist]\n",
    "\n",
    "counter = Counter(flat_list)\n",
    "\n",
    "most_common_elements = counter.most_common(10)\n",
    "\n",
    "most_common_elements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
